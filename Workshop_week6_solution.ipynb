{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop week 6: Application of Transformers and Syntactic Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Application of Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT for Named Entity Recognition\n",
    "\n",
    "    Using BERT for Named Entity Recognition (NER): \n",
    "    Named Entity Recognition (NER) is a task of identifying and classifying entities in a text into predefined categories such as person, organization, location, time, and others. BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained deep learning model that has shown state-of-the-art performance in various natural language processing tasks, including NER. To use BERT for NER, the pre-trained BERT model can be fine-tuned on a labeled dataset of named entities. The fine-tuned BERT model can then be used to predict the named entities in new text. The input to the model is a sequence of tokens, and the output is a sequence of labels that correspond to the named entity categories.\n",
    "\n",
    "\n",
    "\n",
    "BERT has shown superior performance compared to traditional machine learning and deep learning models. Fine-tuning the pre-trained BERT model requires a small labeled dataset and can be done efficiently using transfer learning, making it an effective and efficient approach for various NLP tasks.\n",
    "\n",
    "\n",
    "This part is prepared in a separate as training and testing may take up to half an hour.\n",
    "\n",
    "    Notebook BERT for Named Entity Recognition.ipynb\n",
    "    \n",
    "**The reason for reviewing this code is that it may be useful in your Assignment 2**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Syntactic Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Syntactic parsing is the process of analyzing a sentence or a text in a language and determining its grammatical structure. It involves determining the relationships between the words in a sentence and their roles in building the sentence's meaning.\n",
    "\n",
    "Constituency parsing and Dependency parsing are two approaches used to perform syntactic parsing.\n",
    "\n",
    "Constituency parsing involves analyzing a sentence and determining its constituents, which are the smallest units that make up the sentence. The constituents are organized into a tree structure, where the root of the tree represents the complete sentence, and each branch represents a constituent.\n",
    "\n",
    "Dependency parsing, on the other hand, involves analyzing a sentence and determining the dependencies between its words. A dependency is a relation between two words in a sentence that captures the grammatical role of one word with respect to the other. The dependencies are represented as directed edges in a graph, where each node represents a word in the sentence and each edge represents a dependency between two words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Viterbi algorithm with Probabilistic CFG for syntactic parsing\n",
    "\n",
    "Source: https://www.nltk.org/_modules/nltk/parse/viterbi.html#demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  1: I saw the man with my telescope\n",
      "     <Grammar with 17 productions>\n",
      "\n",
      "  2: the boy saw Jack with Bob under the table with a telescope\n",
      "     <Grammar with 23 productions>\n",
      "\n",
      "Which demo (1-2)?  \n",
      "sent: I saw the man with my telescope\n",
      "parser: <ViterbiParser for <Grammar with 17 productions>>\n",
      "grammar: Grammar with 17 productions (start state = S)\n",
      "    S -> NP VP [1.0]\n",
      "    NP -> Det N [0.5]\n",
      "    NP -> NP PP [0.25]\n",
      "    NP -> 'John' [0.1]\n",
      "    NP -> 'I' [0.15]\n",
      "    Det -> 'the' [0.8]\n",
      "    Det -> 'my' [0.2]\n",
      "    N -> 'man' [0.5]\n",
      "    N -> 'telescope' [0.5]\n",
      "    VP -> VP PP [0.1]\n",
      "    VP -> V NP [0.7]\n",
      "    VP -> V [0.2]\n",
      "    V -> 'ate' [0.35]\n",
      "    V -> 'saw' [0.65]\n",
      "    PP -> P NP [1.0]\n",
      "    P -> 'with' [0.61]\n",
      "    P -> 'under' [0.39]\n",
      "Inserting tokens into the most likely constituents table...\n",
      "   Insert: |=......| I\n",
      "   Insert: |.=.....| saw\n",
      "   Insert: |..=....| the\n",
      "   Insert: |...=...| man\n",
      "   Insert: |....=..| with\n",
      "   Insert: |.....=.| my\n",
      "   Insert: |......=| telescope\n",
      "Finding the most likely constituents spanning 1 text elements...\n",
      "   Insert: |=......| NP -> 'I' [0.15]               0.1500000000 \n",
      "   Insert: |.=.....| V -> 'saw' [0.65]              0.6500000000 \n",
      "   Insert: |.=.....| VP -> V [0.2]                  0.1300000000 \n",
      "   Insert: |..=....| Det -> 'the' [0.8]             0.8000000000 \n",
      "   Insert: |...=...| N -> 'man' [0.5]               0.5000000000 \n",
      "   Insert: |....=..| P -> 'with' [0.61]             0.6100000000 \n",
      "   Insert: |.....=.| Det -> 'my' [0.2]              0.2000000000 \n",
      "   Insert: |......=| N -> 'telescope' [0.5]         0.5000000000 \n",
      "Finding the most likely constituents spanning 2 text elements...\n",
      "   Insert: |==.....| S -> NP VP [1.0]               0.0195000000 \n",
      "   Insert: |..==...| NP -> Det N [0.5]              0.2000000000 \n",
      "   Insert: |.....==| NP -> Det N [0.5]              0.0500000000 \n",
      "Finding the most likely constituents spanning 3 text elements...\n",
      "   Insert: |.===...| VP -> V NP [0.7]               0.0910000000 \n",
      "   Insert: |....===| PP -> P NP [1.0]               0.0305000000 \n",
      "Finding the most likely constituents spanning 4 text elements...\n",
      "   Insert: |====...| S -> NP VP [1.0]               0.0136500000 \n",
      "Finding the most likely constituents spanning 5 text elements...\n",
      "   Insert: |..=====| NP -> NP PP [0.25]             0.0015250000 \n",
      "Finding the most likely constituents spanning 6 text elements...\n",
      "   Insert: |.======| VP -> VP PP [0.1]              0.0002775500 \n",
      "   Insert: |.======| VP -> V NP [0.7]               0.0006938750 \n",
      "  Discard: |.======| VP -> VP PP [0.1]              0.0002775500 \n",
      "Finding the most likely constituents spanning 7 text elements...\n",
      "   Insert: |=======| S -> NP VP [1.0]               0.0001040812 \n",
      "\n",
      "Time (secs)   # Parses   Average P(parse)\n",
      "-----------------------------------------\n",
      "     0.0040          1   0.00010408125000\n",
      "------------------------------------------\n",
      "        n/a          1   0.00010408125000\n",
      "\n",
      "Draw parses (y/n)? n \n",
      "Print parses (y/n)? y (S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (V saw)\n",
      "    (NP\n",
      "      (NP (Det the) (N man))\n",
      "      (PP (P with) (NP (Det my) (N telescope)))))) [0.00010408124999999999]\n"
     ]
    }
   ],
   "source": [
    "def demo(sentence_number=1, draw_parses='y', print_parses='y'):\n",
    "    \"\"\"\n",
    "    A demonstration of the probabilistic parsers.  The user is\n",
    "    prompted to select which demo to run, and how many parses should\n",
    "    be found; and then each parser is run on the same demo, and a\n",
    "    summary of the results are displayed.\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    import time\n",
    "\n",
    "    from functools import reduce\n",
    "    from nltk import tokenize\n",
    "    from nltk.grammar import PCFG\n",
    "    from nltk.parse import ViterbiParser\n",
    "\n",
    "    toy_pcfg1 = PCFG.fromstring(\n",
    "        \"\"\"\n",
    "    S -> NP VP [1.0]\n",
    "    NP -> Det N [0.5] | NP PP [0.25] | 'John' [0.1] | 'I' [0.15]\n",
    "    Det -> 'the' [0.8] | 'my' [0.2]\n",
    "    N -> 'man' [0.5] | 'telescope' [0.5]\n",
    "    VP -> VP PP [0.1] | V NP [0.7] | V [0.2]\n",
    "    V -> 'ate' [0.35] | 'saw' [0.65]\n",
    "    PP -> P NP [1.0]\n",
    "    P -> 'with' [0.61] | 'under' [0.39]\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    toy_pcfg2 = PCFG.fromstring(\n",
    "        \"\"\"\n",
    "    S    -> NP VP         [1.0]\n",
    "    VP   -> V NP          [.59]\n",
    "    VP   -> V             [.40]\n",
    "    VP   -> VP PP         [.01]\n",
    "    NP   -> Det N         [.41]\n",
    "    NP   -> Name          [.28]\n",
    "    NP   -> NP PP         [.31]\n",
    "    PP   -> P NP          [1.0]\n",
    "    V    -> 'saw'         [.21]\n",
    "    V    -> 'ate'         [.51]\n",
    "    V    -> 'ran'         [.28]\n",
    "    N    -> 'boy'         [.11]\n",
    "    N    -> 'cookie'      [.12]\n",
    "    N    -> 'table'       [.13]\n",
    "    N    -> 'telescope'   [.14]\n",
    "    N    -> 'hill'        [.5]\n",
    "    Name -> 'Jack'        [.52]\n",
    "    Name -> 'Bob'         [.48]\n",
    "    P    -> 'with'        [.61]\n",
    "    P    -> 'under'       [.39]\n",
    "    Det  -> 'the'         [.41]\n",
    "    Det  -> 'a'           [.31]\n",
    "    Det  -> 'my'          [.28]\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    # Define two demos.  Each demo has a sentence and a grammar.\n",
    "    demos = [\n",
    "        (\"I saw the man with my telescope\", toy_pcfg1),\n",
    "        (\"the boy saw Jack with Bob under the table with a telescope\", toy_pcfg2),\n",
    "    ]\n",
    "\n",
    "    # Ask the user which demo they want to use.\n",
    "    print()\n",
    "    for i in range(len(demos)):\n",
    "        print(f\"{i + 1:>3}: {demos[i][0]}\")\n",
    "        print(\"     %r\" % demos[i][1])\n",
    "        print()\n",
    "    print(\"Which demo (%d-%d)? \" % (1, len(demos)), end=\" \")\n",
    "    try:\n",
    "        snum = int(sentence_number) - 1\n",
    "        sent, grammar = demos[snum]\n",
    "    except:\n",
    "        print(\"Bad sentence number\")\n",
    "        return\n",
    "\n",
    "    # Tokenize the sentence.\n",
    "    tokens = sent.split()\n",
    "\n",
    "    parser = ViterbiParser(grammar)\n",
    "    all_parses = {}\n",
    "\n",
    "    print(f\"\\nsent: {sent}\\nparser: {parser}\\ngrammar: {grammar}\")\n",
    "    parser.trace(3)\n",
    "    t = time.time()\n",
    "    parses = parser.parse_all(tokens)\n",
    "    time = time.time() - t\n",
    "    average = (\n",
    "        reduce(lambda a, b: a + b.prob(), parses, 0) / len(parses) if parses else 0\n",
    "    )\n",
    "    num_parses = len(parses)\n",
    "    for p in parses:\n",
    "        all_parses[p.freeze()] = 1\n",
    "\n",
    "    # Print some summary statistics\n",
    "    print()\n",
    "    print(\"Time (secs)   # Parses   Average P(parse)\")\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"%11.4f%11d%19.14f\" % (time, num_parses, average))\n",
    "    parses = all_parses.keys()\n",
    "    if parses:\n",
    "        p = reduce(lambda a, b: a + b.prob(), parses, 0) / len(parses)\n",
    "    else:\n",
    "        p = 0\n",
    "    print(\"------------------------------------------\")\n",
    "    print(\"%11s%11d%19.14f\" % (\"n/a\", len(parses), p))\n",
    "\n",
    "    # Ask the user if we should draw the parses.\n",
    "    print()\n",
    "    print(\"Draw parses (y/n)? \"+draw_parses, end=\" \")\n",
    "    if draw_parses.strip().lower().startswith(\"y\"):\n",
    "        from nltk.draw.tree import draw_trees\n",
    "\n",
    "        print(\"  please wait...\")\n",
    "        draw_trees(*parses)\n",
    "\n",
    "    # Ask the user if we should print the parses.\n",
    "    print()\n",
    "    print(\"Print parses (y/n)? \"+print_parses, end=\" \")\n",
    "    if print_parses.strip().lower().startswith(\"y\"):\n",
    "        for parse in parses:\n",
    "            print(parse)\n",
    "demo(1,'n','y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Task 1: What is the meaning based on this parsing?\n",
    "\n",
    "Analyse the parsing result of the sentence \"I saw the man with my telescope\". As you can figure out, this sentence has two meanings. Based on he parsing result, which meaning of this sentence corresponds the parsing corresponds to? How did you find out?\n",
    "\n",
    "Discuss your findings in the class.\n",
    "\n",
    "### Task 2: Manipulate the probabilities of the CFG grammar to change the meaning.\n",
    "\n",
    "Modify toy_pcfg1 to force the parsing to the other meaning. \n",
    "\n",
    "Are there more than one way to do it?\n",
    "\n",
    "Discuss your finding in the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>Context-free Grammar and Dependency Grammar</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context-Free Grammar (CFG) is a type of formal grammar used to describe the structure of a natural language. A CFG defines a set of rules for generating sentences in a language. Each rule consists of a left-hand side, which is a non-terminal symbol, and a right-hand side, which is a sequence of terminal and non-terminal symbols.\n",
    "\n",
    "A CFG rule has the form:\n",
    "\n",
    "A -> B C D\n",
    "\n",
    "where A is a non-terminal symbol, and B, C, and D are either terminal or non-terminal symbols. The arrow symbol \"->\" represents a production and means that the non-terminal A can be replaced by the sequence of symbols B, C, and D.\n",
    "\n",
    "CFG rules can be used to parse a sentence by constructing a parse tree. The parse tree is a tree structure that represents the syntactic structure of a sentence according to the rules of the grammar. The process of constructing the parse tree involves repeatedly applying the CFG rules to the sentence until all the non-terminal symbols have been replaced by terminal symbols.\n",
    "\n",
    "Dependency Grammar is a type of grammar that defines the dependencies between the words in a sentence. A dependency grammar consists of a set of dependency rules, each of which defines the dependencies between two words in a sentence.\n",
    "\n",
    "A dependency grammar rule has the form:\n",
    "\n",
    "word1 --relation--> word2\n",
    "\n",
    "where word1 and word2 are words in the sentence, and relation is a type of dependency between them. For example, the relationship \"subject\" specifies that word1 is the subject of the sentence, and word2 is the predicate.\n",
    "\n",
    "The accuracy of a dependency parser can be evaluated using various metrics, such as precision, recall, and F1-score. Precision measures the proportion of dependencies that are correctly identified by the parser, while recall measures the proportion of dependencies that are found by the parser compared to the total number of dependencies in the sentence. F1-score is a measure that combines precision and recall and provides a single score that indicates the overall performance of the parser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the sentence \"The cat chased the mouse\".\n",
    "\n",
    "Constituency Parsing:\n",
    "\n",
    "    The sentence can be represented as a constituency tree, where the root of the tree represents the complete sentence, and each branch represents a constituent:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (S <br />\n",
    "   (NP The cat)<br />\n",
    "   (VP chased<br />\n",
    "       (NP the mouse)))<br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependency Parsing:\n",
    "\n",
    "    The sentence can be represented as a dependency graph, where each node represents a word in the sentence and each edge represents a dependency between two words:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  cat --subject--> chased <br />\n",
    "  chased --object--> mouse <br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP (Det the) (N cat))\n",
      "  (VP\n",
      "    (V chased)\n",
      "    (NP (Det a) (N dog))\n",
      "    (PP (IN on) (NP (Det the) (N mat)))))\n",
      "CFG Accuracy: 80.00%\n"
     ]
    }
   ],
   "source": [
    "## Example of CFG\n",
    "\n",
    "import nltk\n",
    "from nltk import CFG\n",
    "from nltk.parse import RecursiveDescentParser\n",
    "from nltk.parse.chart import ChartParser\n",
    "\n",
    "# Define a simple context-free grammar\n",
    "# grammar = CFG.fromstring(\"\"\"\n",
    "# S -> NP VP\n",
    "# NP -> Det N\n",
    "# VP -> V NP\n",
    "# Det -> 'a' | 'the'\n",
    "# N -> 'dog' | 'cat'\n",
    "# V -> 'chased' | 'sat'\n",
    "# \"\"\")\n",
    "\n",
    "# Modify the above code by allowing it to parse \"the cat chased a dog on the mat\"\n",
    "# soln:\n",
    "grammar = CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "NP -> Det N\n",
    "VP -> V NP | V PP | V NP PP\n",
    "PP -> IN NP\n",
    "Det -> 'a' | 'the'\n",
    "N -> 'dog' | 'cat' | 'mat'\n",
    "V -> 'chased' | 'sat'\n",
    "IN -> 'on'\n",
    "\"\"\")\n",
    "\n",
    "# Use a recursive descent parser to parse the sentence\n",
    "rd_parser = RecursiveDescentParser(grammar)\n",
    "sentence = \"the cat chased a dog on the mat\"\n",
    "tokens = sentence.split()\n",
    "for tree in rd_parser.parse(tokens):\n",
    "    print(tree)\n",
    "\n",
    "# Evaluate the accuracy of the CFG using a chart parser\n",
    "chart_parser = ChartParser(grammar)\n",
    "test_sentences = [\n",
    "    \"the cat chased a dog\",\n",
    "    \"a dog chased the cat\",\n",
    "    \"the dog sat\",\n",
    "    \"a cat chased the dog\",\n",
    "    \"the cat sat on the mat\"\n",
    "]\n",
    "correct = 0\n",
    "total = len(test_sentences)\n",
    "for sentence in test_sentences:\n",
    "    tokens = sentence.split()\n",
    "    parse_trees = chart_parser.parse(tokens)\n",
    "    if len(list(parse_trees)) > 0:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / total\n",
    "print(\"CFG Accuracy: {:.2f}%\".format(accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Modify the above code by allowing it to parse \"the cat chased a dog on the mat\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency and constituency parsing with spacy\n",
    "\n",
    "\n",
    "In this example, we first load the English language model from spaCy. Then, we parse a sample sentence and print the dependencies between the tokens. Finally, we evaluate the accuracy of the parser by parsing several test sentences and counting the number of correct parses. The accuracy is calculated as the ratio of correct parses to the total number of test sentences. The accuracy is determined by checking if the noun chunks were correctly extracted from the test sentences, but other metrics could be used for evaluation as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP (CD One) (NN morning)) (NP (PRP I)) (VP (VBD chased) (NP (DT a) (NN cat)) (PP (IN in) (NP (PRP$ my) (NNS pyjamas)))))\n",
      "                            S                              \n",
      "      ______________________|_______                        \n",
      "     |           |                  VP                     \n",
      "     |           |     _____________|_______                \n",
      "     |           |    |         |           PP             \n",
      "     |           |    |         |        ___|____           \n",
      "     NP          NP   |         NP      |        NP        \n",
      "  ___|_____      |    |      ___|___    |    ____|_____     \n",
      " CD        NN   PRP  VBD    DT      NN  IN PRP$       NNS  \n",
      " |         |     |    |     |       |   |   |          |    \n",
      "One     morning  I  chased  a      cat  in  my      pyjamas\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# const parsing  \n",
    "# !pip install benepar\n",
    "# !pip install sentencepiece\n",
    "\n",
    "arg_constraints = {} # to stop validation, runs faster\n",
    "import spacy\n",
    "import benepar\n",
    "import en_core_web_sm\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# benepar.download('benepar_en3')\n",
    "\n",
    "\n",
    "# import benepar, spacy\n",
    "import en_core_web_sm\n",
    "from nltk.tree import Tree\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "nlp.add_pipe('benepar', config={\"model\": \"benepar_en3\"})\n",
    "\n",
    "doc = nlp('One morning I chased a cat in my pyjamas')\n",
    "sent = list(doc.sents)[0]\n",
    "str_tree = sent._.parse_string\n",
    "print(str_tree)\n",
    "\n",
    "tree = Tree.fromstring(str_tree)\n",
    "tree.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Meaning of this parser\n",
    "\n",
    "Run the sentence \"I saw the man with my stolen telescope\"\n",
    "\n",
    "Run the sentence \"I saw the man with my own eyes\"\n",
    "\n",
    "\n",
    "Based on the parsing, find out what is the meaning of these sentence. \n",
    "\n",
    "Are these meanings close to the sentence most likely meaning? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"f4105bd0fe4d42c99bb8b17635ef9121-0\" class=\"displacy\" width=\"1625\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">One</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">morning</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">chased</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">cat</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">my</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">pyjamas</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f4105bd0fe4d42c99bb8b17635ef9121-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f4105bd0fe4d42c99bb8b17635ef9121-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f4105bd0fe4d42c99bb8b17635ef9121-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,89.5 570.0,89.5 570.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f4105bd0fe4d42c99bb8b17635ef9121-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f4105bd0fe4d42c99bb8b17635ef9121-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f4105bd0fe4d42c99bb8b17635ef9121-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f4105bd0fe4d42c99bb8b17635ef9121-0-3\" stroke-width=\"2px\" d=\"M770,264.5 C770,177.0 915.0,177.0 915.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f4105bd0fe4d42c99bb8b17635ef9121-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,266.5 L762,254.5 778,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f4105bd0fe4d42c99bb8b17635ef9121-0-4\" stroke-width=\"2px\" d=\"M595,264.5 C595,89.5 920.0,89.5 920.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f4105bd0fe4d42c99bb8b17635ef9121-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M920.0,266.5 L928.0,254.5 912.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f4105bd0fe4d42c99bb8b17635ef9121-0-5\" stroke-width=\"2px\" d=\"M595,264.5 C595,2.0 1100.0,2.0 1100.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f4105bd0fe4d42c99bb8b17635ef9121-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1100.0,266.5 L1108.0,254.5 1092.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f4105bd0fe4d42c99bb8b17635ef9121-0-6\" stroke-width=\"2px\" d=\"M1295,264.5 C1295,177.0 1440.0,177.0 1440.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f4105bd0fe4d42c99bb8b17635ef9121-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,266.5 L1287,254.5 1303,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f4105bd0fe4d42c99bb8b17635ef9121-0-7\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,89.5 1445.0,89.5 1445.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f4105bd0fe4d42c99bb8b17635ef9121-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1445.0,266.5 L1453.0,254.5 1437.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dependency parser\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp('One morning I chased a cat in my pyjamas')\n",
    "displacy.render(doc, style=\"dep\")\n",
    "# displacy.serve(doc, style=\"dep\") # this can be used to display in localhost:5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Interpret the meaning of this sentence from the parsing. \n",
    "\n",
    "Is it easier to find out the meaning based on depencency or constituency parsing?\n",
    "\n",
    "Think of another sentence with ambiguous meaning that is hard to figure out without the background human knowledge.\n",
    "\n",
    "Try it with this parser. Did you get the meaning you expected?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dependency tree for the sentence:\n",
      "The det cat []\n",
      "cat nsubj chased [The]\n",
      "chased ROOT chased [cat, dog, .]\n",
      "the det dog []\n",
      "dog dobj chased [the]\n",
      ". punct chased []\n"
     ]
    }
   ],
   "source": [
    "# Task -2 \n",
    "import spacy\n",
    "\n",
    "# Load the English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define a sentence\n",
    "sentence = \"The cat chased the dog.\"\n",
    "\n",
    "# Parse the sentence to get its grammatical structure\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Print the dependency tree\n",
    "print(\"\\nDependency tree for the sentence:\")\n",
    "for token in doc:\n",
    "    print(token.text, token.dep_, token.head.text, [child for child in token.children])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading: Comparing CFG and Dependency Grammar\n",
    "\n",
    "Context Free Grammar (CFG) and Dependency Grammar are two different approaches to represent the grammatical structure of a sentence.\n",
    "\n",
    "CFG is a type of grammar that consists of a set of production rules that specify the structure of sentences. It defines the relationships between non-terminal symbols and terminal symbols in a sentence. Non-terminal symbols represent parts of speech such as nouns, verbs, adjectives, etc. Terminal symbols represent words in the sentence.\n",
    "\n",
    "Dependency Grammar, on the other hand, represents the grammatical structure of a sentence as a set of dependencies between words in the sentence. It is a type of grammar that defines the relationships between words in a sentence in terms of their function in the sentence. Each word in the sentence is either a dependent or a head. The head is the main word in the relationship, and the dependent is the word that is related to the head.\n",
    "\n",
    "The main difference between CFG and Dependency Grammar is that CFG focuses on the structure of a sentence, while Dependency Grammar focuses on the relationships between words in the sentence. CFG is more suited to generating new sentences based on a set of rules, while Dependency Grammar is more suited to understanding the relationships between words in an existing sentence.\n",
    "\n",
    "The advantages of CFG include its simplicity, generality, and the ability to generate new sentences. The disadvantages include its difficulty in handling free word order and complex relationships between words.\n",
    "\n",
    "The advantages of Dependency Grammar include its ability to handle free word order and complex relationships between words. The disadvantages include its complexity and the difficulty in generating new sentences based on the rules.\n",
    "\n",
    "In conclusion, both CFG and Dependency Grammar have their strengths and weaknesses, and the choice between the two will depend on the specific task at hand.\n",
    "\n",
    "### Optional Task 6: Experimenting with Different Grammars and Dependency Parsers\n",
    "\n",
    "To experiment with different grammars and dependency parsers, you can try using different CFG libraries or implementations, such as the Earley parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Extracting Entities from text: may be useful for Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Entities\n",
    "\n",
    "In natural language processing (NLP), extracting entities refers to the process of identifying and extracting specific pieces of information from text, such as people, organizations, locations, dates, etc. This is a fundamental task in many NLP applications, such as named entity recognition, question answering, information retrieval, and text classification.\n",
    "\n",
    "There are different approaches to extracting entities from text, ranging from rule-based systems to machine learning models. One popular approach is to use pre-trained models such as the ones provided by the spaCy library. These models are trained on large annotated datasets and can achieve high accuracy in identifying and classifying entities.\n",
    "\n",
    "Here's an example of how to extract entities from a text using spaCy in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"Apple is looking at buying a startup for $1 billion\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we load the pre-trained en_core_web_sm model from spaCy and use it to process the text \"Apple is looking at buying a startup for $1 billion\". We then iterate over the identified entities in the doc object and print out their text and label. This shows that the model correctly identified \"Apple\" as an organization and \"$1 billion\" as a monetary value. Steps:\n",
    "\n",
    "We first import the spaCy library and load a pre-trained model for the English language using nlp = spacy.load(\"en_core_web_sm\"). This initializes an instance of the Language class and loads the pre-trained model data for English.\n",
    "\n",
    "\n",
    "We then define the input text that we want to extract entities from using text = \"Apple is looking at buying a startup for $1 billion\".\n",
    "\n",
    "\n",
    "Next, we use the nlp object to process the input text by calling doc = nlp(text). \n",
    "\n",
    "This creates a Doc object that contains various linguistic annotations such as part-of-speech tags, dependencies, and named entities.\n",
    "\n",
    "\n",
    "Finally, we iterate over the entities identified in the input text using a for loop and print out their text and label using print(ent.text, ent.label_). The ent variable represents an individual entity in the doc object, and ent.text and ent.label_ return the text and label of the entity, respectively.\n",
    "\n",
    "**Apply the above code to one of the articles from your assignment 2. Check if working.** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "957e73fbdacff42aadf8c40baf5b66edb7e1c8efd15eaad2479fa1452a07e511"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
