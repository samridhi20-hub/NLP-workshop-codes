{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop week 12:Text Generation and Summarisation\n",
    "\n",
    "### Introduction to Text Generation using GPT-2\n",
    "\n",
    "Text generation stands as one of the most useful applications of Natural Language Processing.\n",
    "Decode based GPT is the still the SOTA of text generation and other NLP tasks. \n",
    "Although we cannot use the latest GPT-4, we are going to use GPT-2, which is a smaller pre-trained model that can be run with limited resources.\n",
    "\n",
    "## Activity 1: GPT-2 Open Text Generation\n",
    "\n",
    "Work on this activity is groups (one at each table)\n",
    "\n",
    "1. Review the following code to understand its working\n",
    "2. Think of a few more prompting examples, and generate texts using them\n",
    "3. Open ChatGPT-3.5 and use the same examples to generate texts\n",
    "4. Compare GPT-2 with GPT-3.5 generation applying human evaluation criteria discussed in Lecture 11. Apply scoring from 1 to 5 for each criteria, add scored together to comare the models.\n",
    "    fluency\n",
    "    coherence / consistency\n",
    "    factuality and correctness\n",
    "    commonsense\n",
    "    style / formality\n",
    "    grammaticality\n",
    "    typicality (what type of something, exemplars etc.)\n",
    "    redundancy\n",
    "5. Discuss your findings in the class. What are the variations between different groups in the class in evaluating texts?\n",
    "\n",
    "**Explanation of code:**\n",
    "\n",
    "    Tokenizer Initialization: The code initializes a GPT-2 tokenizer (tokenizer) to preprocess text inputs. Tokenizers break down input text into tokens, which are numerical representations used by the model.\n",
    "\n",
    "    Model Initialization: The GPT-2 model (model) is loaded. This model is a pre-trained neural network that has learned to predict the next word in a sequence given some context.\n",
    "\n",
    "    Maximum Length: max_length is set to control the length of the generated text. This prevents the model from generating excessively long responses.\n",
    "\n",
    "    Input Prompt: The prompt variable contains the initial snippet of text provided to the model for text generation.\n",
    "\n",
    "    Encoding the Input: The encode() method of the tokenizer converts the input prompt into token IDs (input_ids). These token IDs are the numerical representations of the input text.\n",
    "\n",
    "    Text Generation: The generate() method of the GPT-2 model generates text based on the input token IDs (input_ids). The do_sample=True parameter allows for sampling from the model's predicted probability distribution, adding randomness to the generated text.\n",
    "\n",
    "    Decoding the Output: The decode() method of the tokenizer converts the generated token IDs (output_ids) back into text, excluding any special tokens such as padding or separator tokens.\n",
    "\n",
    "    Printing the Output: The generated text (output_text) is printed to the console for visualization.\n",
    "\n",
    "This code demonstrates the process of using GPT-2 for text generation based on an initial prompt, providing participants with a hands-on understanding of how the model operates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d7245d7436843b8b6bc3b2a204093c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6edd4eba509343af8f715211fe4a02cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03163b1c4d094922b3d056c817e4e748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b122845225b4b62b2844dd7552087d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d1e5d246144e35be3ff677d9429393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/523M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox is a little different than the rest, only slightly tougher. The brown fox will eat even if it encounters the most obvious predator. There is definitely a difference between brown and brown fox, even though both have very similar prey for their prey. It's like they are constantly trying to find their mates, for some reason.\n",
      "\n",
      "The brown fox is the same as the brown fox, while the brown fox and the brown fox are very similar. The brown fox is the same as\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load the GPT-2 tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Set the maximum length of the generated text\n",
    "max_length = 100\n",
    "\n",
    "# Define the input prompt\n",
    "prompt = \"The quick brown fox\"\n",
    "\n",
    "# Encode the input prompt using the tokenizer\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "# Generate the text using the GPT-2 model\n",
    "output_ids = model.generate(input_ids=input_ids, max_length=max_length, do_sample=True)\n",
    "\n",
    "# Decode the generated text using the tokenizer\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated text\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 2: Text Summarisation\n",
    "\n",
    "The following code can summarise text using Bart and trasformer pipeline.\n",
    "\n",
    "1. Review the example code below\n",
    "2. In the second cell of code, implement article summarisation, both abstractive and extractive, from given short news. \n",
    "3. Compare these two types of summarisation using ROUGE, as well as human evaluation as in Activity 2.\n",
    "4. Answer the following questions:\n",
    "    \n",
    "    a. Which type of summarisation generally gives better ROUGE score?\n",
    "    \n",
    "    b. Which type of summarisation generally gives better human score?\n",
    "    \n",
    " Discuss the results in the class. If you find these articles hard to assess the quality of summarisation, you can use some articles from your assignment 2, but need to provide a reference summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text summarisation example\n",
    "\n",
    "!pip install rouge\n",
    "\n",
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from transformers import pipeline\n",
    "from rouge import Rouge\n",
    "\n",
    "# Load the BART tokenizer and model for abstractive summarization\n",
    "tokenizer_abstractive = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "model_abstractive = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "# Load the pipeline for extractive summarization\n",
    "pipeline_extractive = pipeline('summarization')\n",
    "\n",
    "# Define the input text\n",
    "input_text = \"The quick brown fox jumps over the lazy dog. This is a test sentence for summarization. Here is another sentence for testing.\"\n",
    "\n",
    "# Define the target summary\n",
    "target_summary = \"The quick brown fox jumps over the lazy dog. This is a test sentence for summarization.\"\n",
    "\n",
    "# Perform abstractive summarization using BART\n",
    "inputs = tokenizer_abstractive([input_text], max_length=1024, truncation=True, padding='max_length', return_tensors='pt')\n",
    "outputs = model_abstractive.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=60, num_beams=4, length_penalty=2.0)\n",
    "summary_abstractive = tokenizer_abstractive.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Perform extractive summarization using pipeline\n",
    "summary_extractive = pipeline_extractive(input_text, max_length=60)[0]['summary_text']\n",
    "\n",
    "# Evaluate the summaries using the ROUGE metric\n",
    "rouge = Rouge()\n",
    "scores_abstractive = rouge.get_scores(summary_abstractive, target_summary)\n",
    "scores_extractive = rouge.get_scores(summary_extractive, target_summary)\n",
    "\n",
    "# Print the summaries and ROUGE scores\n",
    "print(\"Input Text: \", input_text)\n",
    "print(\"Target Summary: \", target_summary)\n",
    "print(\"Abstractive Summary: \", summary_abstractive)\n",
    "print(\"ROUGE Scores for Abstractive Summary: \", scores_abstractive)\n",
    "print(\"Extractive Summary: \", summary_extractive)\n",
    "print(\"ROUGE Scores for Extractive Summary: \", scores_extractive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text summarisation example\n",
    "\n",
    "# !pip install rouge\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from transformers import pipeline\n",
    "from rouge import Rouge\n",
    "\n",
    "# Load the CNN/DailyMail dataset\n",
    "df = pd.read_csv('./daily_cnn.csv')\n",
    "\n",
    "for all_articles_in_file:\n",
    "    ...\n",
    "    # Print the summaries and ROUGE scores\n",
    "    print(\"Input Text: \", input_text)\n",
    "    print(\"Target Summary: \", target_summary)\n",
    "    print(\"Abstractive Summary: \", summary_abstractive)\n",
    "    print(\"ROUGE Scores for Abstractive Summary: \", scores_abstractive)\n",
    "    print(\"Extractive Summary: \", summary_extractive)\n",
    "    print(\"ROUGE Scores for Extractive Summary: \", scores_extractive)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
