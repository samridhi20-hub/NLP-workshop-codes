{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop week 6: Application of Transformers and Syntactic Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Application of Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT for Named Entity Recognition\n",
    "\n",
    "    Using BERT for Named Entity Recognition (NER): \n",
    "    Named Entity Recognition (NER) is a task of identifying and classifying entities in a text into predefined categories such as person, organization, location, time, and others. BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained deep learning model that has shown state-of-the-art performance in various natural language processing tasks, including NER. To use BERT for NER, the pre-trained BERT model can be fine-tuned on a labeled dataset of named entities. The fine-tuned BERT model can then be used to predict the named entities in new text. The input to the model is a sequence of tokens, and the output is a sequence of labels that correspond to the named entity categories.\n",
    "\n",
    "\n",
    "\n",
    "BERT has shown superior performance compared to traditional machine learning and deep learning models. Fine-tuning the pre-trained BERT model requires a small labeled dataset and can be done efficiently using transfer learning, making it an effective and efficient approach for various NLP tasks.\n",
    "\n",
    "\n",
    "This part is prepared in a separate as training and testing may take up to half an hour.\n",
    "\n",
    "    Notebook BERT for Named Entity Recognition.ipynb\n",
    "    \n",
    "**The reason for reviewing this code is that it may be useful in your Assignment 2**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Syntactic Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Syntactic parsing is the process of analyzing a sentence or a text in a language and determining its grammatical structure. It involves determining the relationships between the words in a sentence and their roles in building the sentence's meaning.\n",
    "\n",
    "Constituency parsing and Dependency parsing are two approaches used to perform syntactic parsing.\n",
    "\n",
    "Constituency parsing involves analyzing a sentence and determining its constituents, which are the smallest units that make up the sentence. The constituents are organized into a tree structure, where the root of the tree represents the complete sentence, and each branch represents a constituent.\n",
    "\n",
    "Dependency parsing, on the other hand, involves analyzing a sentence and determining the dependencies between its words. A dependency is a relation between two words in a sentence that captures the grammatical role of one word with respect to the other. The dependencies are represented as directed edges in a graph, where each node represents a word in the sentence and each edge represents a dependency between two words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Viterbi algorithm with Probabilistic CFG for syntactic parsing\n",
    "\n",
    "Source: https://www.nltk.org/_modules/nltk/parse/viterbi.html#demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo(sentence_number=1, draw_parses='y', print_parses='y'):\n",
    "    \"\"\"\n",
    "    A demonstration of the probabilistic parsers.  The user is\n",
    "    prompted to select which demo to run, and how many parses should\n",
    "    be found; and then each parser is run on the same demo, and a\n",
    "    summary of the results are displayed.\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    import time\n",
    "\n",
    "    from functools import reduce\n",
    "    from nltk import tokenize\n",
    "    from nltk.grammar import PCFG\n",
    "    from nltk.parse import ViterbiParser\n",
    "\n",
    "    toy_pcfg1 = PCFG.fromstring(\n",
    "        \"\"\"\n",
    "    S -> NP VP [1.0]\n",
    "    NP -> Det N [0.5] | NP PP [0.25] | 'John' [0.1] | 'I' [0.15]\n",
    "    Det -> 'the' [0.8] | 'my' [0.2]\n",
    "    N -> 'man' [0.5] | 'telescope' [0.5]\n",
    "    VP -> VP PP [0.1] | V NP [0.7] | V [0.2]\n",
    "    V -> 'ate' [0.35] | 'saw' [0.65]\n",
    "    PP -> P NP [1.0]\n",
    "    P -> 'with' [0.61] | 'under' [0.39]\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    toy_pcfg2 = PCFG.fromstring(\n",
    "        \"\"\"\n",
    "    S    -> NP VP         [1.0]\n",
    "    VP   -> V NP          [.59]\n",
    "    VP   -> V             [.40]\n",
    "    VP   -> VP PP         [.01]\n",
    "    NP   -> Det N         [.41]\n",
    "    NP   -> Name          [.28]\n",
    "    NP   -> NP PP         [.31]\n",
    "    PP   -> P NP          [1.0]\n",
    "    V    -> 'saw'         [.21]\n",
    "    V    -> 'ate'         [.51]\n",
    "    V    -> 'ran'         [.28]\n",
    "    N    -> 'boy'         [.11]\n",
    "    N    -> 'cookie'      [.12]\n",
    "    N    -> 'table'       [.13]\n",
    "    N    -> 'telescope'   [.14]\n",
    "    N    -> 'hill'        [.5]\n",
    "    Name -> 'Jack'        [.52]\n",
    "    Name -> 'Bob'         [.48]\n",
    "    P    -> 'with'        [.61]\n",
    "    P    -> 'under'       [.39]\n",
    "    Det  -> 'the'         [.41]\n",
    "    Det  -> 'a'           [.31]\n",
    "    Det  -> 'my'          [.28]\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    # Define two demos.  Each demo has a sentence and a grammar.\n",
    "    demos = [\n",
    "        (\"I saw the man with my telescope\", toy_pcfg1),\n",
    "        (\"the boy saw Jack with Bob under the table with a telescope\", toy_pcfg2),\n",
    "    ]\n",
    "\n",
    "    # Ask the user which demo they want to use.\n",
    "    print()\n",
    "    for i in range(len(demos)):\n",
    "        print(f\"{i + 1:>3}: {demos[i][0]}\")\n",
    "        print(\"     %r\" % demos[i][1])\n",
    "        print()\n",
    "    print(\"Which demo (%d-%d)? \" % (1, len(demos)), end=\" \")\n",
    "    try:\n",
    "        snum = int(sentence_number) - 1\n",
    "        sent, grammar = demos[snum]\n",
    "    except:\n",
    "        print(\"Bad sentence number\")\n",
    "        return\n",
    "\n",
    "    # Tokenize the sentence.\n",
    "    tokens = sent.split()\n",
    "\n",
    "    parser = ViterbiParser(grammar)\n",
    "    all_parses = {}\n",
    "\n",
    "    print(f\"\\nsent: {sent}\\nparser: {parser}\\ngrammar: {grammar}\")\n",
    "    parser.trace(3)\n",
    "    t = time.time()\n",
    "    parses = parser.parse_all(tokens)\n",
    "    time = time.time() - t\n",
    "    average = (\n",
    "        reduce(lambda a, b: a + b.prob(), parses, 0) / len(parses) if parses else 0\n",
    "    )\n",
    "    num_parses = len(parses)\n",
    "    for p in parses:\n",
    "        all_parses[p.freeze()] = 1\n",
    "\n",
    "    # Print some summary statistics\n",
    "    print()\n",
    "    print(\"Time (secs)   # Parses   Average P(parse)\")\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"%11.4f%11d%19.14f\" % (time, num_parses, average))\n",
    "    parses = all_parses.keys()\n",
    "    if parses:\n",
    "        p = reduce(lambda a, b: a + b.prob(), parses, 0) / len(parses)\n",
    "    else:\n",
    "        p = 0\n",
    "    print(\"------------------------------------------\")\n",
    "    print(\"%11s%11d%19.14f\" % (\"n/a\", len(parses), p))\n",
    "\n",
    "    # Ask the user if we should draw the parses.\n",
    "    print()\n",
    "    print(\"Draw parses (y/n)? \"+draw_parses, end=\" \")\n",
    "    if draw_parses.strip().lower().startswith(\"y\"):\n",
    "        from nltk.draw.tree import draw_trees\n",
    "\n",
    "        print(\"  please wait...\")\n",
    "        draw_trees(*parses)\n",
    "\n",
    "    # Ask the user if we should print the parses.\n",
    "    print()\n",
    "    print(\"Print parses (y/n)? \"+print_parses, end=\" \")\n",
    "    if print_parses.strip().lower().startswith(\"y\"):\n",
    "        for parse in parses:\n",
    "            print(parse)\n",
    "demo(1,'n','y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Task 1: What is the meaning based on this parsing?\n",
    "\n",
    "Analyse the parsing result of the sentence \"I saw the man with my telescope\". As you can figure out, this sentence has two meanings. Based on he parsing result, which meaning of this sentence corresponds the parsing corresponds to? How did you find out?\n",
    "\n",
    "Discuss your findings in the class.\n",
    "\n",
    "### Task 2: Manipulate the probabilities of the CFG grammar to change the meaning.\n",
    "\n",
    "Modify toy_pcfg1 to force the parsing to the other meaning. \n",
    "\n",
    "Are there more than one way to do it?\n",
    "\n",
    "Discuss your finding in the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>Context-free Grammar and Dependency Grammar</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context-Free Grammar (CFG) is a type of formal grammar used to describe the structure of a natural language. A CFG defines a set of rules for generating sentences in a language. Each rule consists of a left-hand side, which is a non-terminal symbol, and a right-hand side, which is a sequence of terminal and non-terminal symbols.\n",
    "\n",
    "A CFG rule has the form:\n",
    "\n",
    "A -> B C D\n",
    "\n",
    "where A is a non-terminal symbol, and B, C, and D are either terminal or non-terminal symbols. The arrow symbol \"->\" represents a production and means that the non-terminal A can be replaced by the sequence of symbols B, C, and D.\n",
    "\n",
    "CFG rules can be used to parse a sentence by constructing a parse tree. The parse tree is a tree structure that represents the syntactic structure of a sentence according to the rules of the grammar. The process of constructing the parse tree involves repeatedly applying the CFG rules to the sentence until all the non-terminal symbols have been replaced by terminal symbols.\n",
    "\n",
    "Dependency Grammar is a type of grammar that defines the dependencies between the words in a sentence. A dependency grammar consists of a set of dependency rules, each of which defines the dependencies between two words in a sentence.\n",
    "\n",
    "A dependency grammar rule has the form:\n",
    "\n",
    "word1 --relation--> word2\n",
    "\n",
    "where word1 and word2 are words in the sentence, and relation is a type of dependency between them. For example, the relationship \"subject\" specifies that word1 is the subject of the sentence, and word2 is the predicate.\n",
    "\n",
    "The accuracy of a dependency parser can be evaluated using various metrics, such as precision, recall, and F1-score. Precision measures the proportion of dependencies that are correctly identified by the parser, while recall measures the proportion of dependencies that are found by the parser compared to the total number of dependencies in the sentence. F1-score is a measure that combines precision and recall and provides a single score that indicates the overall performance of the parser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the sentence \"The cat chased the mouse\".\n",
    "\n",
    "Constituency Parsing:\n",
    "\n",
    "    The sentence can be represented as a constituency tree, where the root of the tree represents the complete sentence, and each branch represents a constituent:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  (S <br />\n",
    "   (NP The cat)<br />\n",
    "   (VP chased<br />\n",
    "       (NP the mouse)))<br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependency Parsing:\n",
    "\n",
    "    The sentence can be represented as a dependency graph, where each node represents a word in the sentence and each edge represents a dependency between two words:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  cat --subject--> chased <br />\n",
    "  chased --object--> mouse <br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP (Det the) (N cat)) (VP (V chased) (NP (Det a) (N dog))))\n",
      "CFG Accuracy: 75.00%\n"
     ]
    }
   ],
   "source": [
    "## Example of CFG\n",
    "\n",
    "import nltk\n",
    "from nltk import CFG\n",
    "from nltk.parse import RecursiveDescentParser\n",
    "from nltk.parse.chart import ChartParser\n",
    "\n",
    "#  Define a simple context-free grammar\n",
    "grammar = CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "NP -> Det N\n",
    "VP -> V NP\n",
    "Det -> 'a' | 'the'\n",
    "N -> 'dog' | 'cat'\n",
    "V -> 'chased' | 'sat'\n",
    "\"\"\")\n",
    "\n",
    "# Use a recursive descent parser to parse the sentence\n",
    "rd_parser = RecursiveDescentParser(grammar)\n",
    "sentence = \"the cat chased a dog\"\n",
    "tokens = sentence.split()\n",
    "for tree in rd_parser.parse(tokens):\n",
    "    print(tree)\n",
    "\n",
    "# Evaluate the accuracy of the CFG using a chart parser\n",
    "chart_parser = ChartParser(grammar)\n",
    "test_sentences = [\n",
    "    \"the cat chased a dog\",\n",
    "    \"a dog chased the cat\",\n",
    "    \"the dog sat\",\n",
    "    \"a cat chased the dog\",\n",
    "]\n",
    "correct = 0\n",
    "total = len(test_sentences)\n",
    "for sentence in test_sentences:\n",
    "    tokens = sentence.split()\n",
    "    parse_trees = chart_parser.parse(tokens)\n",
    "    if len(list(parse_trees)) > 0:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / total\n",
    "print(\"CFG Accuracy: {:.2f}%\".format(accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Modify the above code by allowing it to parse \"the cat chased a dog on the mat\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency and constituency parsing with spacy\n",
    "\n",
    "\n",
    "In this example, we first load the English language model from spaCy. Then, we parse a sample sentence and print the dependencies between the tokens. Finally, we evaluate the accuracy of the parser by parsing several test sentences and counting the number of correct parses. The accuracy is calculated as the ratio of correct parses to the total number of test sentences. The accuracy is determined by checking if the noun chunks were correctly extracted from the test sentences, but other metrics could be used for evaluation as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting benepar\n",
      "  Downloading benepar-0.2.0.tar.gz (33 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: nltk>=3.2 in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from benepar) (3.7)\n",
      "Requirement already satisfied: spacy>=2.0.9 in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from benepar) (3.7.2)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from benepar) (2.2.1)\n",
      "Collecting torch-struct>=0.5\n",
      "  Downloading torch_struct-0.5-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: tokenizers>=0.9.4 in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from benepar) (0.15.2)\n",
      "Requirement already satisfied: transformers[tokenizers,torch]>=4.2.2 in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from benepar) (4.38.2)\n",
      "Collecting protobuf\n",
      "  Downloading protobuf-4.25.3-cp39-cp39-win_amd64.whl (413 kB)\n",
      "     ------------------------------------- 413.4/413.4 kB 30.2 kB/s eta 0:00:00\n",
      "Collecting sentencepiece>=0.1.91\n",
      "  Downloading sentencepiece-0.2.0-cp39-cp39-win_amd64.whl (991 kB)\n",
      "     ------------------------------------ 991.5/991.5 kB 186.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: click in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from nltk>=3.2->benepar) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from nltk>=3.2->benepar) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from nltk>=3.2->benepar) (4.64.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from nltk>=3.2->benepar) (1.1.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from spacy>=2.0.9->benepar) (63.4.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from spacy>=2.0.9->benepar) (1.0.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from spacy>=2.0.9->benepar) (1.21.5)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from spacy>=2.0.9->benepar) (8.2.3)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from spacy>=2.0.9->benepar) (0.3.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from spacy>=2.0.9->benepar) (3.0.9)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from spacy>=2.0.9->benepar) (2.0.10)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from spacy>=2.0.9->benepar) (2.28.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from spacy>=2.0.9->benepar) (1.1.2)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from spacy>=2.0.9->benepar) (0.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from spacy>=2.0.9->benepar) (21.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from spacy>=2.0.9->benepar) (2.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from spacy>=2.0.9->benepar) (2.4.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from spacy>=2.0.9->benepar) (3.0.12)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from spacy>=2.0.9->benepar) (5.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from spacy>=2.0.9->benepar) (2.11.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from spacy>=2.0.9->benepar) (1.0.10)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from spacy>=2.0.9->benepar) (2.6.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from spacy>=2.0.9->benepar) (3.3.0)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from tokenizers>=0.9.4->benepar) (0.21.4)\n",
      "Requirement already satisfied: networkx in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from torch>=1.6.0->benepar) (2.8.4)\n",
      "Requirement already satisfied: sympy in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from torch>=1.6.0->benepar) (1.10.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from torch>=1.6.0->benepar) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from torch>=1.6.0->benepar) (4.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from torch>=1.6.0->benepar) (3.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (6.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (0.4.2)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (0.27.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from accelerate>=0.21.0->transformers[tokenizers,torch]>=4.2.2->benepar) (5.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy>=2.0.9->benepar) (3.0.9)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.0.9->benepar) (2.16.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.0.9->benepar) (0.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (2.0.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy>=2.0.9->benepar) (0.1.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy>=2.0.9->benepar) (0.7.11)\n",
      "Requirement already satisfied: colorama in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from tqdm->nltk>=3.2->benepar) (0.4.6)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy>=2.0.9->benepar) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from jinja2->spacy>=2.0.9->benepar) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (from sympy->torch>=1.6.0->benepar) (1.2.1)\n",
      "Building wheels for collected packages: benepar\n",
      "  Building wheel for benepar (setup.py): started\n",
      "  Building wheel for benepar (setup.py): finished with status 'done'\n",
      "  Created wheel for benepar: filename=benepar-0.2.0-py3-none-any.whl size=37625 sha256=809bbefd59394cbb0db1e75b097743c06672e80fe85123f73edb9a75b125b88e\n",
      "  Stored in directory: c:\\users\\samridhi\\appdata\\local\\pip\\cache\\wheels\\dc\\9a\\8b\\5d4c83fde00b8d068594afa8dd3907809f592e019423df533f\n",
      "Successfully built benepar\n",
      "Installing collected packages: sentencepiece, protobuf, torch-struct, benepar\n",
      "Successfully installed benepar-0.2.0 protobuf-4.25.3 sentencepiece-0.2.0 torch-struct-0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\samridhi\\anaconda3\\lib\\site-packages (0.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package benepar_en3 to\n",
      "[nltk_data]     C:\\Users\\samridhi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Error downloading 'benepar_en3' from\n",
      "[nltk_data]     <https://github.com/nikitakit/self-attentive-\n",
      "[nltk_data]     parser/releases/download/models/benepar_en3.zip>:\n",
      "[nltk_data]     [WinError 10053] An established connection was aborted\n",
      "[nltk_data]     by the software in your host machine\n"
     ]
    },
    {
     "ename": "BadZipFile",
     "evalue": "File is not a zip file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17808\\2850645862.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'benepar'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"benepar_en3\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'One morning I chased a cat in my pyjamas'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36madd_pipe\u001b[1;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[0;32m    819\u001b[0m             )\n\u001b[0;32m    820\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 821\u001b[1;33m             pipe_component = self.create_pipe(\n\u001b[0m\u001b[0;32m    822\u001b[0m                 \u001b[0mfactory_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    823\u001b[0m                 \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36mcreate_pipe\u001b[1;34m(self, factory_name, name, config, raw_config, validate)\u001b[0m\n\u001b[0;32m    707\u001b[0m         \u001b[1;31m# We're calling the internal _fill here to avoid constructing the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m         \u001b[1;31m# registered functions twice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m         \u001b[0mresolved\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregistry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    710\u001b[0m         \u001b[0mfilled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregistry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"cfg\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfactory_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"cfg\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m         \u001b[0mfilled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\confection\\__init__.py\u001b[0m in \u001b[0;36mresolve\u001b[1;34m(cls, config, schema, overrides, validate)\u001b[0m\n\u001b[0;32m    757\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m     ) -> Dict[str, Any]:\n\u001b[1;32m--> 759\u001b[1;33m         resolved, _ = cls._make(\n\u001b[0m\u001b[0;32m    760\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresolve\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\confection\\__init__.py\u001b[0m in \u001b[0;36m_make\u001b[1;34m(cls, config, schema, overrides, resolve, validate)\u001b[0m\n\u001b[0;32m    806\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_interpolated\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 808\u001b[1;33m         filled, _, resolved = cls._fill(\n\u001b[0m\u001b[0;32m    809\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresolve\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresolve\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\confection\\__init__.py\u001b[0m in \u001b[0;36m_fill\u001b[1;34m(cls, config, schema, validate, resolve, parent, overrides)\u001b[0m\n\u001b[0;32m    878\u001b[0m                     \u001b[1;31m# We don't want to try/except this and raise our own error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m                     \u001b[1;31m# here, because we want the traceback if the function fails.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m                     \u001b[0mgetter_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m                     \u001b[1;31m# We're not resolving and calling the function, so replace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\benepar\\integrations\\spacy_plugin.py\u001b[0m in \u001b[0;36mcreate_benepar_component\u001b[1;34m(nlp, name, model, subbatch_max_tokens, disable_tagger)\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[0mdisable_tagger\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m ):\n\u001b[1;32m--> 176\u001b[1;33m     return BeneparComponent(\n\u001b[0m\u001b[0;32m    177\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[0msubbatch_max_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubbatch_max_tokens\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\benepar\\integrations\\spacy_plugin.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, subbatch_max_tokens, disable_tagger, batch_size)\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdeprecated\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mignored\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0muse\u001b[0m \u001b[0msubbatch_max_tokens\u001b[0m \u001b[0minstead\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \"\"\"\n\u001b[1;32m--> 116\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_trained_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\benepar\\integrations\\downloader.py\u001b[0m in \u001b[0;36mload_trained_model\u001b[1;34m(model_name_or_path)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_trained_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mmodel_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlocate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_chart\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mChartParser\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mChartParser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_trained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\benepar\\integrations\\downloader.py\u001b[0m in \u001b[0;36mlocate_model\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[0mnltk_loc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"models/{name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mnltk_loc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    553\u001b[0m             \u001b[0mmodified_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpieces\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpieces\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".zip\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpieces\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodified_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    556\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    540\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m                     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m                         \u001b[1;32mreturn\u001b[0m \u001b[0mZipFilePathPointer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzipentry\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m                         \u001b[1;31m# resource not in zipfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\compat.py\u001b[0m in \u001b[0;36m_decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0minit_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_decorator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, zipfile, entry)\u001b[0m\n\u001b[0;32m    392\u001b[0m         \"\"\"\n\u001b[0;32m    393\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzipfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m             \u001b[0mzipfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOpenOnDemandZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzipfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[1;31m# Check that the entry exists:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\compat.py\u001b[0m in \u001b[0;36m_decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0minit_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_decorator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m    933\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ReopenableZipFile filename must be a string\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 935\u001b[1;33m         \u001b[0mzipfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    936\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    937\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[0;32m   1264\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1266\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_RealGetContents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1267\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'x'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1268\u001b[0m                 \u001b[1;31m# set the modified flag so central directory gets written\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\zipfile.py\u001b[0m in \u001b[0;36m_RealGetContents\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1331\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"File is not a zip file\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1332\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mendrec\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1333\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"File is not a zip file\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1334\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1335\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendrec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBadZipFile\u001b[0m: File is not a zip file"
     ]
    }
   ],
   "source": [
    "# const parsing  \n",
    "!pip install benepar\n",
    "!pip install sentencepiece\n",
    "\n",
    "arg_constraints = {} # to stop validation, runs faster\n",
    "import spacy\n",
    "import benepar\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "benepar.download('benepar_en3')\n",
    "\n",
    "\n",
    "import benepar, spacy\n",
    "import en_core_web_sm\n",
    "from nltk.tree import Tree\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "nlp.add_pipe('benepar', config={\"model\": \"benepar_en3\"})\n",
    "\n",
    "doc = nlp('One morning I chased a cat in my pyjamas')\n",
    "sent = list(doc.sents)[0]\n",
    "str_tree = sent._.parse_string\n",
    "print(str_tree)\n",
    "\n",
    "tree = Tree.fromstring(str_tree)\n",
    "tree.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Meaning of this parser\n",
    "\n",
    "Run the sentence \"I saw the man with my stolen telescope\"\n",
    "\n",
    "Run the sentence \"I saw the man with my own eyes\"\n",
    "\n",
    "\n",
    "Based on the parsing, find out what is the meaning of these sentence. \n",
    "\n",
    "Are these meanings close to the sentence most likely meaning? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependency parser\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp('One morning I chased a cat in my pyjamas')\n",
    "displacy.render(doc, style=\"dep\")\n",
    "# displacy.serve(doc, style=\"dep\") # this can be used to display in localhost:5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Interpret the meaning of this sentence from the parsing. \n",
    "\n",
    "Is it easier to find out the meaning based on depencency or constituency parsing?\n",
    "\n",
    "Think of another sentence with ambiguous meaning that is hard to figure out without the background human knowledge.\n",
    "\n",
    "Try it with this parser. Did you get the meaning you expected?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Task -2 \n",
    "import spacy\n",
    "\n",
    "# Load the English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define a sentence\n",
    "sentence = \"The cat chased the dog.\"\n",
    "\n",
    "# Parse the sentence to get its grammatical structure\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Print the dependency tree\n",
    "print(\"\\nDependency tree for the sentence:\")\n",
    "for token in doc:\n",
    "    print(token.text, token.dep_, token.head.text, [child for child in token.children])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading: Comparing CFG and Dependency Grammar\n",
    "\n",
    "Context Free Grammar (CFG) and Dependency Grammar are two different approaches to represent the grammatical structure of a sentence.\n",
    "\n",
    "CFG is a type of grammar that consists of a set of production rules that specify the structure of sentences. It defines the relationships between non-terminal symbols and terminal symbols in a sentence. Non-terminal symbols represent parts of speech such as nouns, verbs, adjectives, etc. Terminal symbols represent words in the sentence.\n",
    "\n",
    "Dependency Grammar, on the other hand, represents the grammatical structure of a sentence as a set of dependencies between words in the sentence. It is a type of grammar that defines the relationships between words in a sentence in terms of their function in the sentence. Each word in the sentence is either a dependent or a head. The head is the main word in the relationship, and the dependent is the word that is related to the head.\n",
    "\n",
    "The main difference between CFG and Dependency Grammar is that CFG focuses on the structure of a sentence, while Dependency Grammar focuses on the relationships between words in the sentence. CFG is more suited to generating new sentences based on a set of rules, while Dependency Grammar is more suited to understanding the relationships between words in an existing sentence.\n",
    "\n",
    "The advantages of CFG include its simplicity, generality, and the ability to generate new sentences. The disadvantages include its difficulty in handling free word order and complex relationships between words.\n",
    "\n",
    "The advantages of Dependency Grammar include its ability to handle free word order and complex relationships between words. The disadvantages include its complexity and the difficulty in generating new sentences based on the rules.\n",
    "\n",
    "In conclusion, both CFG and Dependency Grammar have their strengths and weaknesses, and the choice between the two will depend on the specific task at hand.\n",
    "\n",
    "### Optional Task 6: Experimenting with Different Grammars and Dependency Parsers\n",
    "\n",
    "To experiment with different grammars and dependency parsers, you can try using different CFG libraries or implementations, such as the Earley parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Extracting Entities from text: may be useful for Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Entities\n",
    "\n",
    "In natural language processing (NLP), extracting entities refers to the process of identifying and extracting specific pieces of information from text, such as people, organizations, locations, dates, etc. This is a fundamental task in many NLP applications, such as named entity recognition, question answering, information retrieval, and text classification.\n",
    "\n",
    "There are different approaches to extracting entities from text, ranging from rule-based systems to machine learning models. One popular approach is to use pre-trained models such as the ones provided by the spaCy library. These models are trained on large annotated datasets and can achieve high accuracy in identifying and classifying entities.\n",
    "\n",
    "Here's an example of how to extract entities from a text using spaCy in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"Apple is looking at buying a startup for $1 billion\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we load the pre-trained en_core_web_sm model from spaCy and use it to process the text \"Apple is looking at buying a startup for $1 billion\". We then iterate over the identified entities in the doc object and print out their text and label. This shows that the model correctly identified \"Apple\" as an organization and \"$1 billion\" as a monetary value. Steps:\n",
    "\n",
    "We first import the spaCy library and load a pre-trained model for the English language using nlp = spacy.load(\"en_core_web_sm\"). This initializes an instance of the Language class and loads the pre-trained model data for English.\n",
    "\n",
    "\n",
    "We then define the input text that we want to extract entities from using text = \"Apple is looking at buying a startup for $1 billion\".\n",
    "\n",
    "\n",
    "Next, we use the nlp object to process the input text by calling doc = nlp(text). \n",
    "\n",
    "This creates a Doc object that contains various linguistic annotations such as part-of-speech tags, dependencies, and named entities.\n",
    "\n",
    "\n",
    "Finally, we iterate over the entities identified in the input text using a for loop and print out their text and label using print(ent.text, ent.label_). The ent variable represents an individual entity in the doc object, and ent.text and ent.label_ return the text and label of the entity, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply the above code to one of the articles from your assignment 2. Check if working.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "957e73fbdacff42aadf8c40baf5b66edb7e1c8efd15eaad2479fa1452a07e511"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
