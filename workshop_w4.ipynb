{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c15ce706",
   "metadata": {},
   "source": [
    "## Workshop: POS tagging, Named Entity Recognition, Vanishing gradient\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b2dec1",
   "metadata": {},
   "source": [
    "### Part of Speech (POS) Tagging\n",
    "\n",
    "POS Tagging is the process of marking each word in a text with its corresponding part of speech (noun, verb, adjective, etc.). This information can be used for various NLP tasks such as sentiment analysis, named entity recognition, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff704f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'JJ'), ('text', 'NN'), ('for', 'IN'), ('POS', 'NNP'), ('Tagging', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Importing required libraries\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Downloading required resources\n",
    "# nltk.download(\"punkt\")\n",
    "# nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "# Defining a function to perform POS Tagging\n",
    "def pos_tagging(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged_words = nltk.pos_tag(tokens)\n",
    "    return tagged_words\n",
    "\n",
    "# Testing the model on a sample text\n",
    "text = \"This is a sample text for POS Tagging.\"\n",
    "tagged_words = pos_tagging(text)\n",
    "print(tagged_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42368cd6",
   "metadata": {},
   "source": [
    "This code tokenizes the text using the word_tokenize function and then performs POS Tagging using the pos_tag function from the nltk library. The pos_tag function returns a list of tuples, where each tuple consists of a token and its corresponding part of speech tag.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b8bd19",
   "metadata": {},
   "source": [
    "### Task 1: POS tagging with spacy\n",
    "\n",
    "spacy package is designed to perform many useful NLP tasks, such as tokenization, tagging, NER, lemmatization and more.\n",
    "\n",
    "Implementing POS with spacy package is even simpler than with NLTK.\n",
    "Find relevant information in spacy documentation.\n",
    "\n",
    "#### Task: Try a few sentences with more than one meaning, e.g. \"I can fish\", \"Time flies like an arrow\". Answer the following questions: Wich meaning of the sentence the tagging is done for? Is this the most likely meaning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226c29c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "print(\"token.text, token.lemma_, token.pos_, token.tag_\")\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4597f48",
   "metadata": {},
   "source": [
    "### Task 2: Implement Hidden Markov Model (HMM) for POS Tagging using NLTK\n",
    "\n",
    "HMM is a statistical model that can be used to perform POS Tagging. An HMM model is trained on a tagged corpus, and it uses the probabilities of word transitions to predict the part of speech tags for a given text.\n",
    "\n",
    "This code first tokenizes the sentence into words using the nltk.word_tokenize function, then trains a Hidden Markov Model POS tagger using the nltk.hmm.HiddenMarkovModelTagger.train method and the Penn Treebank corpus. Finally, it uses the trained tagger to perform the POS tagging of the sentence and prints the resulting tags. The output will be a list of tuples, where each tuple consists of a word and its corresponding POS tag.\n",
    "\n",
    "##### Task: Complete the following code to implement HMM tagging with NLTK. Try sentences that you have tried in Task 1 and compare the results. Are results for more difficult sentences the same or different? Which tagger is better or more accurate?\n",
    "\n",
    "Use NLTK docmumentation to complete the code https://www.nltk.org/_modules/nltk/tag/hmm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afccc7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('treebank') # done first time\n",
    "\n",
    "# Define the sample sentence\n",
    "sentence = \"I love playing soccer in the park\"\n",
    "\n",
    "# Tokenize the sentence into words\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Use the HMM POS tagger from the nltk library\n",
    "hmm_tagger = <your code>\n",
    "\n",
    "# Perform the POS tagging\n",
    "pos_tags = <your code>\n",
    "\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef13700e",
   "metadata": {},
   "source": [
    "### Task 3: Named Entity Recognition (NER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cef9a6",
   "metadata": {},
   "source": [
    "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP) to identify and extract named entities such as people, organizations, locations, and other entities from unstructured text. NER is used to extract relevant information from large amounts of text data, which can be used for various purposes such as information retrieval, question answering, and text summarization.\n",
    "\n",
    "#### Task: Try to add missing code to complete NER tagging below. Refer to spacy documentation for help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b020625b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"\"\"In ancient Rome, some neighbors live in three adjacent houses. In the center is the house of Senex, who lives there with wife Domina, son Hero, and several slaves, including head slave Hysterium and the musical's main character Pseudolus. A slave belonging to Hero, Pseudolus wishes to buy, win, or steal his freedom. One of the neighboring houses is owned by Marcus Lycus, who is a buyer and seller of beautiful women; the other belongs to the ancient Erronius, who is abroad searching for his long-lost children (stolen in infancy by pirates). One day, Senex and Domina go on a trip and leave Pseudolus in charge of Hero. Hero confides in Pseudolus that he is in love with the lovely Philia, one of the courtesans in the House of Lycus (albeit still a virgin).\"\"\"\n",
    "\n",
    "<add code here>\n",
    "\n",
    "displacy.render(sentence_spans, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc659582",
   "metadata": {},
   "source": [
    "### Task 3: Vanishing gradient\n",
    "\n",
    "The followig code demostrates vanishing gradient problem for sigmoid activation function.\n",
    "\n",
    "In this simple code, we’re storing the mean of the gradients at each epoch in a list. After the training loop, we plot this list to visualize how the gradient changes over time. You’ll notice that the gradient values decrease over time, demonstrating the vanishing gradient problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d5a2c7",
   "metadata": {},
   "source": [
    "##### Task: Add more activation functions\n",
    "\n",
    "Modify the code to implement two more activation functions: ReLU, and Leaky ReLU.\n",
    "Run the code and compare gradient vanishing: which activation function keep gradient for more epochs? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911dd41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sigmoid, tanh and relu functions and their derivatives\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(y):\n",
    "    return 1 - y**2\n",
    "\n",
    "def relu(x):\n",
    "    <your code here>\n",
    "\n",
    "def relu_derivative(y):\n",
    "    <your code here>\n",
    "\n",
    "leak_value = 0.001\n",
    "def leaky_relu(x):\n",
    "    <your code here>\n",
    "\n",
    "def leaky_relu_derivative(x):\n",
    "    <your code here>\n",
    "\n",
    "# size of layers\n",
    "inputLayer_size = 2\n",
    "hiddenLayer_size = 3\n",
    "outputLayer_size = 1\n",
    "\n",
    "# weights initialization\n",
    "weights_ih = np.random.uniform(size=(inputLayer_size, hiddenLayer_size))\n",
    "weights_ho = np.random.uniform(size=(hiddenLayer_size, outputLayer_size))\n",
    "\n",
    "# inputs and outputs\n",
    "inputs = np.array([[0.5, 0.6]])\n",
    "outputs = np.array([[0.1]])\n",
    "\n",
    "# learning rate\n",
    "lr = 0.01\n",
    "\n",
    "# store gradients\n",
    "gradients_sigmoid = []\n",
    "gradients_tanh = []\n",
    "gradients_relu = []\n",
    "gradients_leaky_relu = []\n",
    "\n",
    "# training loop for each activation function\n",
    "for activation, derivative, gradients in [(sigmoid, sigmoid_derivative, gradients_sigmoid), \n",
    "                                          (tanh, tanh_derivative, gradients_tanh), \n",
    "                                          (relu, relu_derivative, gradients_relu),\n",
    "                                          (leaky_relu, leaky_relu_derivative, gradients_leaky_relu)\n",
    "                                         ]:\n",
    "    # reset weights\n",
    "    weights_ih = np.random.uniform(size=(inputLayer_size, hiddenLayer_size))\n",
    "    weights_ho = np.random.uniform(size=(hiddenLayer_size, outputLayer_size))\n",
    "\n",
    "    for epoch in range(5000):\n",
    "        # forward pass\n",
    "        hiddenLayer_input = np.dot(inputs, weights_ih)\n",
    "        hiddenLayer_output = activation(hiddenLayer_input)\n",
    "\n",
    "        outputLayer_input = np.dot(hiddenLayer_output, weights_ho)\n",
    "        outputLayer_output = activation(outputLayer_input)\n",
    "\n",
    "        # backward pass\n",
    "        outputLayer_error = outputs - outputLayer_output\n",
    "        outputLayer_delta = outputLayer_error * derivative(outputLayer_output)\n",
    "\n",
    "        hiddenLayer_error = np.dot(outputLayer_delta, weights_ho.T)\n",
    "        hiddenLayer_delta = hiddenLayer_error * derivative(hiddenLayer_output)\n",
    "\n",
    "        # update weights\n",
    "        weights_ho += lr * np.dot(hiddenLayer_output.T, outputLayer_delta)\n",
    "        weights_ih += lr * np.dot(inputs.T, hiddenLayer_delta)\n",
    "\n",
    "        # store gradient\n",
    "        gradients.append(np.mean(hiddenLayer_delta))\n",
    "\n",
    "# plot gradients\n",
    "plt.title('Vanishing Gradient Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Gradient')\n",
    "plt.plot(gradients_sigmoid, label='Sigmoid')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.title('Vanishing Gradient Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Gradient')\n",
    "plt.plot(gradients_tanh, label='Tanh')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.title('Vanishing Gradient Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Gradient')\n",
    "plt.plot(gradients_relu, label='ReLU')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.title('Vanishing Gradient Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Gradient')\n",
    "plt.plot(gradients_leaky_relu, label='Leaky_ReLU')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
